
import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import logging
import asyncio
from typing import Dict, List, Any
import joblib
from dotenv import load_dotenv

# Add project root to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.config.settings import settings
from supabase import create_client
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score
from ucimlrepo import fetch_ucirepo

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

# ============================================================================
# DATA COLLECTORS
# ============================================================================

class RealDataCollector:
    """Collect real data from multiple sources"""
    
    def __init__(self):
        self.supabase = self._init_supabase()
        
    def _init_supabase(self):
        """Initialize Supabase client"""
        url = os.getenv('VITE_SUPABASE_URL')
        key = os.getenv('VITE_SUPABASE_PUBLISHABLE_KEY')
        if url and key:
            return create_client(url, key)
        logger.warning("Supabase credentials not found")
        return None
    
    async def collect_user_metrics(self) -> pd.DataFrame:
        """
        Collect real user metrics from Supabase
        
        Returns: DataFrame with columns [user_id, metric_date, metric_type, metric_value]
        """
        if not self.supabase:
            logger.warning("Using synthetic data - Supabase not configured")
            return self._generate_synthetic_metrics()
        
        try:
            # Query user_metrics table
            response = self.supabase.table('user_metrics').select('*').limit(10000).execute()
            
            if response.data and len(response.data) > 0:
                df = pd.DataFrame(response.data)
                logger.info(f"‚úì Collected {len(df)} real user metrics from database")
                return df
            else:
                logger.warning("No data in user_metrics table, using synthetic data")
                return self._generate_synthetic_metrics()
                
        except Exception as e:
            logger.error(f"Error fetching user metrics: {e}")
            return self._generate_synthetic_metrics()
    
    async def collect_income_data(self) -> pd.DataFrame:
        """
        Collect income data from multiple sources:
        1. User-reported data from Supabase
        2. Plaid API (if integrated)
        3. Public datasets
        """
        # Try Supabase first
        if self.supabase:
            try:
                response = self.supabase.table('user_metrics').select('*').eq('metric_type', 'income').execute()
                if response.data and len(response.data) > 0:
                    df = pd.DataFrame(response.data)
                    logger.info(f"‚úì Collected {len(df)} income records from database")
                    return self._format_income_data(df)
            except Exception as e:
                logger.error(f"Error fetching income data: {e}")
        
        # Fallback: Load from CSV if available
        csv_path = Path("data/real_income_data.csv")
        if csv_path.exists():
            df = pd.read_csv(csv_path)
            logger.info(f"‚úì Loaded {len(df)} income records from CSV")
            return df
        
        # Final fallback: Enhanced synthetic data
        logger.warning("Using enhanced synthetic income data")
        return self._generate_realistic_income_data()
    
    async def collect_health_data(self) -> pd.DataFrame:
        """
        Collect health metrics from:
        1. Google Fit API
        2. Fitbit API
        3. User-reported data from Supabase
        4. Public health datasets
        """
        # Try Supabase first
        if self.supabase:
            try:
                health_metrics = ['sleep_hours', 'work_hours', 'exercise_minutes', 
                                'mood_score', 'caffeine_cups', 'meetings_count']
                
                response = self.supabase.table('user_metrics').select('*').in_('metric_type', health_metrics).execute()
                
                if response.data and len(response.data) > 0:
                    df = pd.DataFrame(response.data)
                    logger.info(f"‚úì Collected {len(df)} health metrics from database")
                    return self._format_health_data(df)
            except Exception as e:
                logger.error(f"Error fetching health data: {e}")
        
        # Try Google Fit (if credentials available)
        google_fit_data = await self._fetch_google_fit_data()
        if google_fit_data is not None:
            return google_fit_data
        
        # Fallback: Load from CSV
        csv_path = Path("data/real_health_data.csv")
        if csv_path.exists():
            df = pd.read_csv(csv_path)
            logger.info(f"‚úì Loaded {len(df)} health records from CSV")
            return df
        
        logger.warning("Using synthetic health data")
        return self._generate_realistic_health_data()
    
    def collect_uci_adult_data(self) -> pd.DataFrame:
        """Fetch Adult dataset directly from UCI ML Repository"""
        try:
            logger.info("Fetching Adult dataset from UCI Repository (id=2)...")
            adult_repo = fetch_ucirepo(id=2)
            
            X = adult_repo.data.features
            y = adult_repo.data.targets
            
            # Standardize names
            X.columns = [c.replace('-', '_').replace('.', '_').lower() for c in X.columns]
            
            # Combine into single DF
            df = X.copy()
            # Map target to binary
            # UCI target can be ' <=50K', ' >50K', ' <=50K.', ' >50K.'
            df['target'] = y.iloc[:, 0].astype(str).str.strip().str.replace('.', '', regex=False).map({'<=50K': 0, '>50K': 1})
            
            logger.info(f"‚úì Successfully fetched {len(df)} records from UCI Repository")
            return df
        except Exception as e:
            logger.error(f"Error fetching data from UCI Repository: {e}")
            return pd.DataFrame()

    def collect_external_income_data(self) -> pd.DataFrame:
        """
        Collect and preprocess external income datasets for demographic classification.
        Combines direct UCI fetching and Kaggle CSVs.
        """
        dataframes = []
        
        # 1. Direct UCI Fetch
        uci_df = self.collect_uci_adult_data()
        if not uci_df.empty:
            dataframes.append(uci_df)
            
        # 2. Load mastmustu/income (train.csv)
        income_path1 = Path("ml-predicter/data/income/train.csv")
        if income_path1.exists():
            df1 = pd.read_csv(income_path1)
            # Standardize column names
            df1.columns = [c.replace('-', '_').replace('.', '_').lower() for c in df1.columns]
            # Standardize target
            if 'income_>50k' in df1.columns:
                df1['target'] = df1['income_>50k']
            dataframes.append(df1)
            logger.info(f"‚úì Loaded {len(df1)} records from mastmustu/income")
            
        # 3. Load uciml/adult-census-income (adult.csv)
        income_path2 = Path("ml-predicter/data/adult_income/adult.csv")
        if income_path2.exists():
            df2 = pd.read_csv(income_path2)
            # Standardize column names
            df2.columns = [c.replace('-', '_').replace('.', '_').lower() for c in df2.columns]
            # Standardize target
            if 'income' in df2.columns:
                df2['target'] = df2['income'].astype(str).str.strip().str.replace('.', '', regex=False).map({'<=50K': 0, '>50K': 1})
            dataframes.append(df2)
            logger.info(f"‚úì Loaded {len(df2)} records from uciml/adult-census-income")
            
        if not dataframes:
            logger.error("No external income data found")
            return pd.DataFrame()
            
        # Combine
        combined_df = pd.concat(dataframes, ignore_index=True)
        
        # Preprocessing
        combined_df = self._preprocess_demographic_data(combined_df)
        
        logger.info(f"‚úì Successfully combined and preprocessed {len(combined_df)} demographic records")
        return combined_df
    
    def _preprocess_demographic_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and prepare demographic data for ML"""
        # 1. Handle missing values (indicated by '?')
        df = df.replace('?', np.nan)
        
        # Drop rows with missing targets
        if 'target' in df.columns:
            df = df.dropna(subset=['target'])
        
        # Fill missing categorical with mode
        cat_cols = ['workclass', 'education', 'marital_status', 'occupation', 
                    'relationship', 'race', 'gender', 'sex', 'native_country']
        for col in cat_cols:
            if col in df.columns:
                df[col] = df[col].fillna(df[col].mode()[0])
        
        # Standardize sex/gender
        if 'sex' in df.columns and 'gender' not in df.columns:
            df['gender'] = df['sex']
        
        # Select relevant features
        features = [
            'age', 'workclass', 'education', 'marital_status', 'occupation',
            'relationship', 'race', 'gender', 'capital_gain', 'capital_loss',
            'hours_per_week', 'native_country', 'target'
        ]
        
        # Filter to existing features
        available_features = [f for f in features if f in df.columns]
        df = df[available_features]
        
        return df

    async def _fetch_google_fit_data(self):
        """Fetch data from Google Fit API (placeholder)"""
        # TODO: Implement Google Fit OAuth2 flow
        # Requires: google-api-python-client, oauth2client
        return None
    
    def _format_income_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Format raw income data for training"""
        df['metric_date'] = pd.to_datetime(df['metric_date'])
        df = df.sort_values('metric_date')
        
        # Pivot if needed
        if 'metric_type' in df.columns:
            df = df[df['metric_type'] == 'income']
        
        result = pd.DataFrame({
            'date': df['metric_date'],
            'value': df['metric_value']
        })
        return result
    
    def _format_health_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Format raw health data for training"""
        df['metric_date'] = pd.to_datetime(df['metric_date'])
        
        # Pivot wide format
        pivot = df.pivot_table(
            index=['user_id', 'metric_date'],
            columns='metric_type',
            values='metric_value',
            aggfunc='mean'
        ).reset_index()
        
        return pivot
    
    def _generate_synthetic_metrics(self) -> pd.DataFrame:
        """Enhanced synthetic data generation"""
        np.random.seed(42)
        n_users = 50
        days_per_user = 180
        
        data = []
        for user_id in range(n_users):
            base_income = np.random.uniform(40000, 80000)
            for day in range(days_per_user):
                date = datetime(2023, 1, 1) + timedelta(days=day)
                
                # Income with realistic patterns
                seasonal_factor = 1 + 0.1 * np.sin(2 * np.pi * day / 365)
                noise = np.random.normal(0, 0.05)
                income = base_income * seasonal_factor * (1 + noise)
                
                data.append({
                    'user_id': f'user_{user_id}',
                    'metric_date': date,
                    'metric_type': 'income',
                    'metric_value': income
                })
        
        return pd.DataFrame(data)
    
    def _generate_realistic_income_data(self) -> pd.DataFrame:
        """Generate realistic income time series"""
        np.random.seed(42)
        dates = pd.date_range(start='2020-01-01', end='2024-12-31', freq='M')
        
        # Base trend with economic cycles
        trend = np.linspace(45000, 65000, len(dates))
        
        # Seasonal pattern (bonuses in Dec, slow in Jan)
        seasonal = 3000 * np.sin(2 * np.pi * np.arange(len(dates)) / 12)
        
        # Economic shocks (e.g., COVID impact)
        shocks = np.zeros(len(dates))
        covid_start = (pd.Timestamp('2020-03-01') - dates[0]).days // 30
        shocks[covid_start:covid_start+6] = -5000
        
        # Random noise
        noise = np.random.normal(0, 1500, len(dates))
        
        income = trend + seasonal + shocks + noise
        
        return pd.DataFrame({'date': dates, 'value': income})
    
    def _generate_realistic_health_data(self) -> pd.DataFrame:
        """Generate realistic health metrics"""
        np.random.seed(42)
        n_samples = 1000
        
        data = {
            'work_hours': np.random.gamma(8, 0.8, n_samples).clip(4, 14),
            'sleep_hours': np.random.normal(7, 1.2, n_samples).clip(4, 10),
            'exercise_minutes': np.random.exponential(25, n_samples).clip(0, 120),
            'mood_score': np.random.beta(6, 3, n_samples) * 10,
            'caffeine_cups': np.random.poisson(2, n_samples).clip(0, 8),
            'meetings_count': np.random.negative_binomial(4, 0.5, n_samples).clip(0, 15)
        }
        
        df = pd.DataFrame(data)
        
        # Calculate stress score (target variable)
        df['stress_score'] = (
            df['work_hours'] * 5 +
            (10 - df['sleep_hours']) * 8 +
            (60 - df['exercise_minutes']) * 0.5 +
            (10 - df['mood_score']) * 6 +
            df['caffeine_cups'] * 3 +
            df['meetings_count'] * 2 +
            np.random.normal(0, 5, n_samples)
        ).clip(0, 100)
        
        return df


# ============================================================================
# MODEL TRAINER
# ============================================================================

class RealDataModelTrainer:
    """Train models with real data"""
    
    def __init__(self, model_dir: str = "./models"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True, parents=True)
        self.collector = RealDataCollector()
        self.trained_models = {}
    
    async def train_all_models(self):
        """Train all models with real data"""
        logger.info("=" * 70)
        logger.info("TRAINING MODELS WITH REAL DATA")
        logger.info("=" * 70)
        
        # 1. Collect Data
        logger.info("\n[STEP 1/3] Collecting Real Data...")
        income_data = await self.collector.collect_income_data()
        health_data = await self.collector.collect_health_data()
        
        # 2. Train Models
        logger.info("\n[STEP 2/3] Training Models...")
        
        # Income Models
        logger.info("\nüìä Training Income Models...")
        await self.train_income_models(income_data)
        
        # Demographic Model
        logger.info("\nüë§ Training Demographic Income Classifier...")
        demographic_data = self.collector.collect_external_income_data()
        if not demographic_data.empty:
            await self.train_income_demographic_model(demographic_data)
        
        # Health Models
        logger.info("\nüè• Training Health Prediction Models...")
        await self.train_health_models(health_data)
        
        # 3. Evaluate & Save
        logger.info("\n[STEP 3/3] Evaluating Models...")
        self.evaluate_models(income_data, health_data)
        
        logger.info("\n" + "=" * 70)
        logger.info("‚úÖ TRAINING COMPLETED SUCCESSFULLY")
        logger.info("=" * 70)
        logger.info(f"Models saved to: {self.model_dir.absolute()}")
        
        return self.trained_models
    
    async def train_income_models(self, data: pd.DataFrame):
        """Train income forecasting models with real data"""
        
        # Prepare time series
        data = data.sort_values('date')
        values = data['value'].values
        
        # Split train/test
        split_idx = int(len(values) * 0.8)
        train_values = values[:split_idx]
        test_values = values[split_idx:]
        
        # 1. ARIMA Model
        try:
            logger.info("  Training ARIMA...")
            best_model, best_order = self._auto_arima(train_values)
            
            # Test predictions
            predictions = best_model.forecast(steps=len(test_values))
            rmse = np.sqrt(mean_squared_error(test_values, predictions))
            
            joblib.dump(best_model, self.model_dir / "income_arima.pkl")
            self.trained_models['income_arima'] = {
                'order': best_order,
                'rmse': rmse,
                'r2': r2_score(test_values, predictions)
            }
            logger.info(f"  ‚úì ARIMA trained - RMSE: {rmse:.2f}, Order: {best_order}")
            
        except Exception as e:
            logger.error(f"  ‚úó ARIMA training failed: {e}")
        
        # 2. SARIMA Model
        try:
            logger.info("  Training SARIMA...")
            sarima_model = SARIMAX(
                train_values,
                order=(1, 1, 1),
                seasonal_order=(1, 1, 0, 12),
                enforce_stationarity=False,
                enforce_invertibility=False
            ).fit(disp=False)
            
            predictions = sarima_model.forecast(steps=len(test_values))
            rmse = np.sqrt(mean_squared_error(test_values, predictions))
            
            joblib.dump(sarima_model, self.model_dir / "income_sarima.pkl")
            self.trained_models['income_sarima'] = {'rmse': rmse}
            logger.info(f"  ‚úì SARIMA trained - RMSE: {rmse:.2f}")
            
        except Exception as e:
            logger.error(f"  ‚úó SARIMA training failed: {e}")
        
        # 3. Random Forest (with features)
        try:
            logger.info("  Training Random Forest...")
            
            # Engineer features from dates
            data['day_of_week'] = pd.to_datetime(data['date']).dt.dayofweek
            data['month'] = pd.to_datetime(data['date']).dt.month
            data['quarter'] = pd.to_datetime(data['date']).dt.quarter
            
            features = ['day_of_week', 'month', 'quarter']
            X = data[features].values
            y = data['value'].values
            
            # Split
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, shuffle=False
            )
            
            # Scale
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Train
            rf_model = RandomForestRegressor(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                random_state=42,
                n_jobs=-1
            )
            rf_model.fit(X_train_scaled, y_train)
            
            # Evaluate
            predictions = rf_model.predict(X_test_scaled)
            rmse = np.sqrt(mean_squared_error(y_test, predictions))
            r2 = r2_score(y_test, predictions)
            
            joblib.dump(rf_model, self.model_dir / "income_rf.pkl")
            joblib.dump(scaler, self.model_dir / "income_scaler.pkl")
            
            self.trained_models['income_rf'] = {'rmse': rmse, 'r2': r2}
            logger.info(f"  ‚úì Random Forest trained - RMSE: {rmse:.2f}, R¬≤: {r2:.4f}")
            
        except Exception as e:
            logger.error(f"  ‚úó Random Forest training failed: {e}")
    
    async def train_health_models(self, data: pd.DataFrame):
        """Train health prediction models"""
        
        features = ['work_hours', 'sleep_hours', 'exercise_minutes', 
                   'mood_score', 'caffeine_cups', 'meetings_count']
        
        # Ensure all features exist
        for feat in features:
            if feat not in data.columns:
                logger.warning(f"Missing feature: {feat}, using synthetic data")
                return
        
        X = data[features].values
        y_stress = data['stress_score'].values
        
        # Create risk categories
        y_risk = pd.cut(
            data['stress_score'],
            bins=[0, 30, 60, 80, 100],
            labels=[0, 1, 2, 3]
        ).astype(int)
        
        # 1. Stress Classifier
        try:
            logger.info("  Training Stress Classifier...")
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_risk, test_size=0.2, random_state=42
            )
            
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            classifier = GradientBoostingClassifier(
                n_estimators=150,
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            )
            classifier.fit(X_train_scaled, y_train)
            
            accuracy = accuracy_score(y_test, classifier.predict(X_test_scaled))
            
            joblib.dump(classifier, self.model_dir / "health_stress_classifier.pkl")
            joblib.dump(scaler, self.model_dir / "health_stress_scaler.pkl")
            
            self.trained_models['health_stress_classifier'] = {'accuracy': accuracy}
            logger.info(f"  ‚úì Classifier trained - Accuracy: {accuracy:.4f}")
            
        except Exception as e:
            logger.error(f"  ‚úó Classifier training failed: {e}")
        
        # 2. Risk Predictor (Regression)
        try:
            logger.info("  Training Risk Predictor...")
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_stress, test_size=0.2, random_state=42
            )
            
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            predictor = RandomForestRegressor(
                n_estimators=150,
                max_depth=12,
                min_samples_split=5,
                random_state=42,
                n_jobs=-1
            )
            predictor.fit(X_train_scaled, y_train)
            
            predictions = predictor.predict(X_test_scaled)
            rmse = np.sqrt(mean_squared_error(y_test, predictions))
            r2 = r2_score(y_test, predictions)
            
            joblib.dump(predictor, self.model_dir / "health_risk_predictor.pkl")
            joblib.dump(scaler, self.model_dir / "health_risk_scaler.pkl")
            
            self.trained_models['health_risk_predictor'] = {'rmse': rmse, 'r2': r2}
            logger.info(f"  ‚úì Predictor trained - RMSE: {rmse:.2f}, R¬≤: {r2:.4f}")
            
        except Exception as e:
            logger.error(f"  ‚úó Predictor training failed: {e}")
    
    async def train_income_demographic_model(self, data: pd.DataFrame):
        """Train demographic-based income classification model"""
        try:
            logger.info("  Preparing demographic data...")
            
            # Select features for training
            cat_features = ['workclass', 'education', 'marital_status', 'occupation', 
                           'relationship', 'race', 'gender', 'native_country']
            num_features = ['age', 'capital_gain', 'capital_loss', 'hours_per_week']
            
            # Filter to what's available
            available_cat = [f for f in cat_features if f in data.columns]
            available_num = [f for f in num_features if f in data.columns]
            
            # One-hot encode categorical
            X_cat = pd.get_dummies(data[available_cat], drop_first=True)
            X_num = data[available_num]
            X = pd.concat([X_num, X_cat], axis=1)
            y = data['target']
            
            # Save feature names for later use in engine
            feature_names = X.columns.tolist()
            
            # Split
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # Scale
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Train model
            logger.info("  Training Gradient Boosting Classifier...")
            model = GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                random_state=42
            )
            model.fit(X_train_scaled, y_train)
            
            # Evaluate
            y_pred = model.predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
            
            # Save model, scaler and feature metadata
            joblib.dump(model, self.model_dir / "income_demographic_classifier.pkl")
            joblib.dump(scaler, self.model_dir / "income_demographic_scaler.pkl")
            joblib.dump(feature_names, self.model_dir / "income_demographic_features.pkl")
            
            self.trained_models['income_demographic_classifier'] = {
                'accuracy': accuracy,
                'feature_count': len(feature_names)
            }
            logger.info(f"  ‚úì Demographic Classifier trained - Accuracy: {accuracy:.4f}")
            
        except Exception as e:
            logger.error(f"  ‚úó Demographic Classifier training failed: {e}")

    def _auto_arima(self, values, max_p=3, max_d=2, max_q=3):
        """Automatic ARIMA order selection"""
        best_aic = float('inf')
        best_model = None
        best_order = (1, 1, 1)
        
        for p in range(max_p):
            for d in range(max_d):
                for q in range(max_q):
                    try:
                        model = ARIMA(values, order=(p, d, q))
                        fitted = model.fit()
                        if fitted.aic < best_aic:
                            best_aic = fitted.aic
                            best_model = fitted
                            best_order = (p, d, q)
                    except:
                        continue
        
        return best_model, best_order
    
    def evaluate_models(self, income_data, health_data):
        """Generate evaluation report"""
        logger.info("\nüìà Model Performance Summary:")
        logger.info("-" * 70)
        
        for model_name, metrics in self.trained_models.items():
            logger.info(f"\n{model_name}:")
            for metric, value in metrics.items():
                if isinstance(value, float):
                    logger.info(f"  {metric}: {value:.4f}")
                else:
                    logger.info(f"  {metric}: {value}")


# ============================================================================
# MAIN EXECUTION
# ============================================================================

async def main():
    """Main training pipeline"""
    trainer = RealDataModelTrainer(model_dir=settings.MODEL_PATH)
    
    try:
        models = await trainer.train_all_models()
        
        print("\n" + "=" * 70)
        print("üéØ NEXT STEPS:")
        print("=" * 70)
        print("1. Models are now trained with real data")
        print("2. Test predictions: pytest tests/")
        print("3. Deploy: Update forecast_engine.py to use these models")
        print("4. Monitor: Track prediction accuracy over time")
        print("=" * 70)
        
    except Exception as e:
        logger.error(f"Training pipeline failed: {e}", exc_info=True)
        sys.exit(1)


if __name__ == '__main__':
    asyncio.run(main())